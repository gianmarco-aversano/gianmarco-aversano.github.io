# Test-Driven Development

There is no robust, bug-free code without good tests. Tests make sure that any problems are caught before they reach PROD.

Test-Driven Development (TDD) is an approach where tests are written before the actual code.

In TDD, a developer writes a failing unit test for a new feature or change, then writes just enough code to pass that test (and, optionally, refactors the code while ensuring the test continues to pass).

And the cycle repeats.

## How does this differ from traditional testing?

Traditional testing tests after coding is done.
You code something, then write tests.
This is (almost) completely useless, because writing tests after coding only ensures that the tests agree with what the code already does.
Testing after coding does not challenge any choice that you made!
And if you try to challenge your code, you may end up having to re-write it from zero...

In traditional testing, the tests do not _drive_ development, they just what already exists.

## Why writing tests before coding

No code should (ever) be written if there is no use-case in mind.
More code means more bugs, so we must write code only when it solves a real problem or requirement.
We can risk writing code (bugs) when the trade-off is worth it: we write more code, but we solve something in return.

How can we tell if our code is solving a problem? Only by writing tests **first**.

The development workflow should look something like this:

`Understand logic or requirements` -> `understand how to prove requirements are satisfied` -> `code`.

Thus, it is important that:

- You understand the requirements and are able to write down, in your own natural language, what exactly needs to be done. Master the problem.
- You understand how to prove the requirements are satisfied. How can you tell that this new feature actually works? When are you happy with it? Will you trust it blindly? If you do not know how to answer, no coding will help you.
- You are able to write code (tests) that proves the new functionality is there. You must **trust** your tests (thus, they must be good). In the future, the code will grow, will change and impact old code. The tests will tell you if you are breaking anything. If they pass, feel free to merge to PROD 2 minutes before going on a holiday.

> The tests must show you all possibilities that can happen in PROD, and even more. There must be nothing that can happen in PROD that is not covered by the tests.

Finally, writing tests before coding allows you to desire whatever you want from your code. Do not limit your code to what you think is possible, write any test, write demanding tests, desire anything, then code to make it happen.

## TDD: green light, red light

The rules are simple. Start with a test, write just enough so that the test can fail.
Only when the test fails (red), you can code.
Write enough code just to pass the test, not one word more!
If the test passes (green), you cannot write more code. Go back to write more tests.

Rinse and repeat.

## What does this look like in practice?

Let's say we want to implement a new feature.

The first thing we need are the requirements:

```python
"""
Build a string manipulator with the following requirements:
- convert to lower case
- remove a defined pattern from the string
"""
```

Without something as clear as this (you can see it's a Python docstring already), there is no going forward. If youre requirements are too vague or too broad, either re-align with managers and/or colleagues or break it down.

Then, we incorporate this into a test and write the test:

```python
def test_convert_lower_case() -> None:
    """Test our string manipulator.

    We want to ensure that the following requirements are met:
    - Input `str` is converted to lower case.
    - A defined pattern is removed from the string.

    We will prove that requirements are met by testing the following cases:
    0. If input is not a `str`, raise any error.
    1. Input `str` has upper-case characters; output `str` only has lower-case.
    2. Input `str` has the predefined patter; output `str` does not have it.
    3. Input `str` is already lower-case and does not have the predefined pattern; output `str` is equal to input.
    """
```

You can see there there is no code, just the test set up. You can also see that our ideas are super clear about what feature we want to implement and how we are planning to test that it works.

Now that the plan is clear, we can write the tests.

Yes, before writing the code. This will ensure that the test _drives_ the code: this will prevent us from writing code that is too complex, or code that is hard to use.

### Step 1: decide how we should import your new code

> Remember [this section](#tdd-green-light-red-light).

The first thing we have to do is to decide how the test has to import the function or class to test.
This will force you to make this resource easily accessible and not come up with complex stuff like `from this.that.now_this._private._okhere import _weird_name`.
Imagine if `numpy` or `pandas` forced you to import their code like that... Choose something simple.

```python
from mylib import manipulate_string

def test_convert_lower_case() -> None:
    """ < same - as - above > """
```

Have we written enough test? Let's run this test -> it fails because the resource `manipulate_string` does not exist.

Good. Now we can code.

We create the following:

```python
# Create this file in a way so that it satisfies the import in the test
def manipulate_string() -> str:
    """<describe>"""
```

Stop there. Does the test run and pass? Well, yes, this is enough code to pass the test.
So go back to testing.

### Step 2: write some more test

We can import the function now, let's use it:

```python
from mylib import manipulate_string

def test_convert_lower_case() -> None:
    """ < same - as - above > """
    out = manipulate_string("PYTEST")
    assert out == "pytest"
```

If you run this test, it fails because the function `manipulate_string` does not accept any argument yet.
So we can code!

Update the code:

```python
def manipulate_string(x: str) -> str:
    """<describe>"""
```

Stop. Run the test, does it pass? The error now will be different: the runtime will complain that the function returned `None` and, thus, is not equal to our target value `"pytest"`.
So we must continue to code.

Update the code:

```python
def manipulate_string(x: str) -> str:
    """<describe>"""
    return x.lower()
```

This will make the test pass!

So no more coding. Back to the test.

### Step 3: raise error if input is invalid

This was one of the requirements. Are we able to translate this into code? Yes.

```python
import pytest
from mylib import manipulate_string

def test_convert_lower_case() -> None:
    """ < same - as - above > """
    out = manipulate_string("PYTEST")
    assert out == "pytest"

    with pytest.raises(Exception):
        manipulate_string(None)
```

This test won't pass until we do:

```python
def manipulate_string(x: str) -> str:
    """<describe>"""
    if not isinstance(x, str):
        raise Exception("Invalid input.")
    return x.lower()
```

Now it will pass, we go back to testing.

And so on. Until code meets all requirements.

## Bad tests, good tests

In the example above, you can see how we are testing behavior, not implementation.

The same test would pass also with this code:

```python
from pydantic import validate_call, ConfigDict

@validate_call(
    config=ConfigDict(validate_assignment=True, validate_default=True),
    validate_return=True,
)
def manipulate_string(x: str) -> str:
    """<describe>"""
    return x.lower()
```

which solution is better? It does not matter.
The `pydantic`-based solution implicitly brings over more functionalities, but the test is not forbidding them, so both solutions are ok.

### Test behavior, not implementation

This is important because we want to test behavior, independent from implementation.
If you tightly couple tests to implementations, classes, etc., then any refactoring is impossible.
Tests will break even if functionalities are the same.

### Test everything against everything

Does this function accept a `str`? Then write tests againt all possible `str` values!
**Especially** meaningless ones.

Good testing starts from testing dummy cases, from testing against non-expected inputs.
Test weirdness first, then go for the actual goal.

Example: this function accepts an `int`... What if I input `None`? What if I input any other type?

Input coverage is very important: test the function against anything that it can accept, and design behavior when something non-expected is inputed.

The function `call_me()` receives a `str` value, but internally it also reads from a database: then try inputting any `str` value, non-`str` values but also mock the database to return valid and non-expected data. What do I want this function to do if the databse returns a table with an extra column? What if a column is missing? What if it is empty? What if a `float` column today returns a `str`?
